import json
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM, get_cosine_schedule_with_warmup
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
import logging
import os
import gc
import time
import random
import numpy as np
from collections import deque
from huggingface_hub import login
from tqdm import tqdm
import shutil
from datetime import datetime

# üöÄ A100 SXM OPTIMIZATIONS - 80GB VRAM + 125GB RAM + 16 vCore
os.environ["TOKENIZERS_PARALLELISM"] = "true"
os.environ["OMP_NUM_THREADS"] = "16"
os.environ["MKL_NUM_THREADS"] = "16"
os.environ["TRANSFORMERS_VERBOSITY"] = "error"
os.environ["CUDA_LAUNCH_BLOCKING"] = "0"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
torch.set_num_threads(16)

# A100 PCIe GPU performance optimizations
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

# Hugging Face Token
HF_TOKEN = "your_huggingface_token_here"  # Replace with your HuggingFace API token

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/your/path/here/sft_training.log'),  # Replace with your log file path
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# UPDATED CONFIG - SFT-only training
TRAINING_CONFIG = {
    "learning_rate": 2e-5,
    "batch_size": 8,
    "gradient_accumulation_steps": 6,  # Effective: 48
    "num_epochs": 3,
    "max_length": 2048,
    "warmup_ratio": 0.03,
    "weight_decay": 0.01,
    "max_grad_norm": 1.0,
    "seed": 42,
    "eval_frequency": 5,  # 5 evaluations per epoch
    "logging_steps": 10,
    "dataloader_num_workers": 8,
    "pin_memory": True,
    "dataloader_persistent_workers": True,
    "prefetch_factor": 4,
    
    # Early stopping config
    "early_stopping_patience": 3,
    "early_stopping_min_delta": 0.001,
    
    # Checkpoint strategy
    "checkpoint_dir": "/your/path/here/llama3.2-3b-sft-training",  # Replace with your checkpoint directory
    "save_best_model": True,
    "save_last_checkpoint": True,
    "save_every_n_epochs": 1,
    "save_every_n_steps": 5000,
    "keep_last_n_checkpoints": 2,
    
    # Resume training
    "resume_from_checkpoint": "/your/path/here/llama3.2-3b-sft-training/checkpoints/checkpoint-epoch-1.pt"  # Replace with your checkpoint path or set to None
}

class SFTDataset(Dataset):
    """SFT-only dataset"""
    
    def __init__(self, data, tokenizer, max_length=2048):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        logger.info("üéØ Creating SFT Dataset:")
        logger.info(f"   Total samples: {len(data)}")
        
        # Pre-tokenize all data
        self.pre_tokenize_all_data()
        
    def pre_tokenize_all_data(self):
        """Pre-tokenize all samples for performance"""
        logger.info("üî• PRE-TOKENIZING ALL SAMPLES...")
        
        self.tokenized_cache = {}
        
        for i, item in enumerate(tqdm(self.data, desc="Tokenizing")):
            text = self.create_sft_prompt(item)
            
            tokens = self.tokenizer(
                text,
                truncation=True,
                max_length=self.max_length,
                padding=False,
                return_tensors=None,
                add_special_tokens=True
            )
            
            self.tokenized_cache[i] = {
                'input_ids': tokens['input_ids'],
                'attention_mask': tokens['attention_mask']
            }
        
        logger.info(f"‚úÖ Tokenization complete: {len(self.tokenized_cache)} entries")
    
    def create_sft_prompt(self, item):
        """Create SFT prompt"""
        nl_desc = item['nl']
        cleaned = (nl_desc
                  .replace("concode_field_sep", " | ")
                  .replace("concode_elem_sep", ", ")
                  .strip())
        cleaned = ' '.join(cleaned.split())
            
        prompt = f"""You are an expert Java programmer. Generate a complete, working Java method for the given description.

Task: {cleaned}

Requirements:
- Write a complete Java method
- Use proper syntax and naming conventions
- Include return statements where needed
- Keep it concise but functional

```java
"""
        
        return prompt + item['code'].strip() + "\n```" + self.tokenizer.eos_token
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        """Return pre-tokenized data"""
        return self.tokenized_cache[idx]

class SimplifiedEarlyStopping:
    """Early stopping that tracks average loss only (for SFT-only training)"""
    
    def __init__(self, patience=3, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        
        self.best_loss = float('inf')
        self.counter = 0
        
        self.best_model_state = None
        self.best_epoch = 0
        self.best_step = 0
        
    def update(self, avg_loss, model_state, epoch, step):
        """Update early stopping counter"""
        # Check improvement
        if self.best_loss - avg_loss > self.min_delta:
            self.counter = 0
            self.best_loss = avg_loss
            self.best_epoch = epoch
            self.best_step = step
            return True  # New best model
        else:
            self.counter += 1
            return False  # Not a new best
    
    def should_stop(self):
        """Check if loss stopped improving"""
        return self.counter >= self.patience
    
    def get_status(self):
        """Get current early stopping status"""
        return {
            'patience': f"{self.counter}/{self.patience}",
            'best_loss': self.best_loss,
            'best_epoch': self.best_epoch,
            'best_step': self.best_step
        }

class SimpleLossTracker:
    """Loss tracking for SFT-only training"""
    
    def __init__(self, window_size=250):
        self.window_size = window_size
        self.losses = deque(maxlen=window_size)
        self.step_count = 0
        
    def update(self, loss_values):
        """Update with batch losses"""
        self.losses.extend(loss_values)
        self.step_count += 1
    
    def get_recent_average(self):
        """Get recent average"""
        return sum(self.losses) / len(self.losses) if self.losses else 0.0

class A100SFTTrainer:
    """A100 SFT-Only Trainer"""
    
    def __init__(self, model_name="meta-llama/Llama-3.2-3B", base_path="/your/base/path"):  # Replace base_path with your directory
        self.model_name = model_name
        self.base_path = base_path
        self.tokenizer = None
        self.model = None
        self.optimizer = None
        self.scheduler = None
        
        self.loss_tracker = SimpleLossTracker(window_size=250)
        self.early_stopping = SimplifiedEarlyStopping(
            patience=TRAINING_CONFIG["early_stopping_patience"],
            min_delta=TRAINING_CONFIG["early_stopping_min_delta"]
        )
        
        # Memory management
        self.memory_threshold = 70.0
        self.cleanup_frequency = 20
        
        # Create checkpoint directory
        self.checkpoint_dir = TRAINING_CONFIG["checkpoint_dir"]
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        os.makedirs(os.path.join(self.checkpoint_dir, "checkpoints"), exist_ok=True)
        os.makedirs(os.path.join(self.checkpoint_dir, "best"), exist_ok=True)
        os.makedirs(os.path.join(self.checkpoint_dir, "final"), exist_ok=True)
        
    def setup_model_and_tokenizer(self):
        """Load model and tokenizer - FIXED FOR NaN ISSUES"""
        logger.info(f"üöÄ Loading model: {self.model_name}")
        
        try:
            login(token=HF_TOKEN)
            logger.info("‚úÖ HF authentication successful")
        except Exception as e:
            logger.warning(f"HF login failed: {e}")
        
        torch.cuda.empty_cache()
        gc.collect()
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            use_fast=True,
            token=HF_TOKEN,
            padding_side="left",
            model_max_length=2048
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
        logger.info(f"Tokenizer loaded - vocab_size: {len(self.tokenizer)}")
        
        # ‚úÖ FIXED MODEL LOADING - USE FLOAT32 FOR STABILITY
        try:
            # Fix transformers compatibility
            import transformers.modeling_utils
            if not hasattr(transformers.modeling_utils, 'ALL_PARALLEL_STYLES') or transformers.modeling_utils.ALL_PARALLEL_STYLES is None:
                transformers.modeling_utils.ALL_PARALLEL_STYLES = ["data_parallel", "model_parallel", "pipeline_parallel", "colwise", "rowwise"]
                logger.info("‚úÖ Fixed ALL_PARALLEL_STYLES")
            
            from transformers import AutoConfig
            config = AutoConfig.from_pretrained(
                self.model_name,
                token=HF_TOKEN,
                trust_remote_code=True
            )
            
            # Clean problematic config
            if hasattr(config, 'tensor_parallel_style'):
                config.tensor_parallel_style = None
            if hasattr(config, 'parallel_attn'):
                config.parallel_attn = False
            if hasattr(config, 'tp_rank'):
                delattr(config, 'tp_rank')
            if hasattr(config, 'tp_size'):
                delattr(config, 'tp_size')
            
            # ‚úÖ USE FLOAT32 INSTEAD OF BFLOAT16
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                config=config,
                torch_dtype=torch.float32,
                device_map="auto",
                trust_remote_code=True,
                token=HF_TOKEN,
                low_cpu_mem_usage=True,
                use_cache=False
            )
            logger.info("‚úÖ Model loaded successfully with float32")
            
        except Exception as e:
            logger.error(f"Model loading failed: {e}")
            # Try alternative loading
            try:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float32,
                    device_map="auto",
                    trust_remote_code=True,
                    token=HF_TOKEN,
                    _attn_implementation="eager"
                )
                logger.info("‚úÖ Model loaded with alternative method (float32)")
            except Exception as e2:
                # Final fallback
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    trust_remote_code=True,
                    token=HF_TOKEN
                )
                if torch.cuda.is_available():
                    self.model = self.model.to('cuda')
                    self.model = self.model.to(torch.float32)
                logger.info("‚úÖ Model loaded with minimal method (float32)")
        
        if hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()
        
        self.model.train()
        
        param_count = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        logger.info(f"‚úÖ Model loaded - {param_count:,} parameters")
        
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"   GPU: {gpu_name} - {vram_total:.1f}GB VRAM")
    
    def prepare_dataset(self, dataset_filename="codexglue_train"):
        """Prepare dataset with train/val split"""
        json_file_path = os.path.join(self.base_path, f"{dataset_filename}.json")
        
        logger.info(f"üìÅ Loading SFT dataset: {json_file_path}")
        
        if not os.path.exists(json_file_path):
            raise FileNotFoundError(f"Dataset not found: {json_file_path}")
        
        with open(json_file_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
        
        logger.info(f"‚úÖ Loaded {len(all_data)} samples")
        
        # Split data - 10% for validation
        random.seed(TRAINING_CONFIG["seed"])
        random.shuffle(all_data)
        
        val_ratio = 0.1
        val_size = int(len(all_data) * val_ratio)
        
        # Split data
        train_data = all_data[val_size:]
        val_data = all_data[:val_size]
        
        logger.info(f"\nüìä Train/Val Split:")
        logger.info(f"üöÇ TRAIN SET: {len(train_data)} samples")
        logger.info(f"üß™ VALIDATION SET: {len(val_data)} samples")
        
        # Show some validation examples
        logger.info("\nüîç Validation Set Examples (first 3):")
        for i, item in enumerate(val_data[:3]):
            logger.info(f"  [{i}] ID: {item.get('id', 'N/A')}")
            logger.info(f"       NL: {item['nl'][:100]}...")
        
        # Create datasets
        train_dataset = SFTDataset(train_data, self.tokenizer, TRAINING_CONFIG["max_length"])
        val_dataset = SFTDataset(val_data, self.tokenizer, TRAINING_CONFIG["max_length"])
        
        return train_dataset, val_dataset
    
    def create_dataloader(self, dataset, is_training=True):
        """Create optimized dataloader"""
        config = TRAINING_CONFIG
        
        def optimized_collate_fn(batch):
            """Optimized collate function"""
            input_ids = [item['input_ids'] for item in batch]
            attention_masks = [item['attention_mask'] for item in batch]
            
            # Dynamic padding
            max_len = max(len(ids) for ids in input_ids)
            batch_size = len(input_ids)
            
            padded_input_ids = torch.full((batch_size, max_len), self.tokenizer.pad_token_id, dtype=torch.long)
            padded_attention_masks = torch.zeros((batch_size, max_len), dtype=torch.long)
            
            for i, (ids, mask) in enumerate(zip(input_ids, attention_masks)):
                seq_len = len(ids)
                padded_input_ids[i, :seq_len] = torch.tensor(ids, dtype=torch.long)
                padded_attention_masks[i, :seq_len] = torch.tensor(mask, dtype=torch.long)
            
            labels = padded_input_ids.clone()
            labels[padded_attention_masks == 0] = -100
            
            return {
                'input_ids': padded_input_ids,
                'attention_mask': padded_attention_masks,
                'labels': labels
            }
        
        return DataLoader(
            dataset,
            batch_size=config["batch_size"],
            shuffle=is_training,
            num_workers=config.get("dataloader_num_workers", 12),
            collate_fn=optimized_collate_fn,
            pin_memory=config.get("pin_memory", True),
            persistent_workers=config.get("dataloader_persistent_workers", True),
            prefetch_factor=config.get("prefetch_factor", 4),
            drop_last=is_training
        )
    
    def setup_optimizer(self, train_dataset):
        """Setup optimizer and scheduler"""
        config = TRAINING_CONFIG
        
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=config["learning_rate"],
            weight_decay=config["weight_decay"],
            eps=1e-8,
            betas=(0.9, 0.95)
        )
        
        # Calculate total steps
        steps_per_epoch = len(train_dataset) // (config["batch_size"] * config["gradient_accumulation_steps"])
        total_steps = steps_per_epoch * config["num_epochs"]
        warmup_steps = int(total_steps * config["warmup_ratio"])
        
        # Calculate eval steps for 5 evaluations per epoch
        self.eval_steps = max(1, steps_per_epoch // config["eval_frequency"])
        
        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps,
            num_cycles=0.5
        )
        
        logger.info(f"üéØ Optimizer setup:")
        logger.info(f"   Steps per epoch: {steps_per_epoch}")
        logger.info(f"   Total steps: {total_steps}")
        logger.info(f"   Warmup steps: {warmup_steps}")
        logger.info(f"   Eval frequency: every {self.eval_steps} steps ({config['eval_frequency']}x per epoch)")
    
    def load_checkpoint(self, checkpoint_path):
        """Load checkpoint for resuming training"""
        logger.info(f"üìÇ Loading checkpoint: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # Load model state
        self.model.load_state_dict(checkpoint['model_state_dict'])
        logger.info("‚úÖ Model state loaded")
        
        # Load optimizer state
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        logger.info("‚úÖ Optimizer state loaded")
        
        # Load scheduler state
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        logger.info("‚úÖ Scheduler state loaded")
        
        # Get training state
        start_epoch = checkpoint['epoch'] + 1  # Start from next epoch
        global_step = checkpoint['global_step']
        
        # Load early stopping state if available
        if 'best_loss' in checkpoint:
            self.early_stopping.best_loss = checkpoint.get('avg_loss', checkpoint.get('best_loss'))
            self.early_stopping.best_epoch = checkpoint.get('best_epoch', checkpoint['epoch'])
            self.early_stopping.best_step = checkpoint.get('best_step', global_step)
            logger.info(f"‚úÖ Early stopping state loaded - Best loss: {self.early_stopping.best_loss:.4f}")
        
        logger.info(f"üìä Resuming from epoch {start_epoch}, step {global_step}")
        
        return start_epoch, global_step
    
    def compute_loss(self, batch):
        """‚úÖ FIXED: Compute loss with robust NaN handling"""
        # Move to device
        if torch.cuda.is_available():
            input_ids = batch['input_ids'].cuda(non_blocking=True)
            attention_mask = batch['attention_mask'].cuda(non_blocking=True)
            labels = batch['labels'].cuda(non_blocking=True)
        else:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']
        
        # ‚úÖ NO MIXED PRECISION - USE FLOAT32 FOR STABILITY
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            use_cache=False
        )
        
        # ‚úÖ ENHANCED per-sample loss computation with NaN protection
        sample_losses = self.compute_per_sample_losses_enhanced(outputs, labels)
        self.loss_tracker.update(sample_losses)
        
        # ‚úÖ FIXED: Always return a valid loss, NEVER skip batches
        combined_loss = outputs.loss
        
        # Enhanced NaN/Inf handling
        if torch.isnan(combined_loss) or torch.isinf(combined_loss) or combined_loss.item() <= 0:
            logger.warning(f"Invalid loss detected: {combined_loss}, using stable fallback")
            # Create a small positive loss as fallback
            combined_loss = torch.tensor(0.5, device=combined_loss.device, dtype=torch.float32, requires_grad=True)
        
        # Clamp loss to reasonable range
        combined_loss = torch.clamp(combined_loss, min=0.01, max=15.0)
        
        return sample_losses, combined_loss
    
    def compute_per_sample_losses_enhanced(self, outputs, labels):
        """‚úÖ ENHANCED: Ultra-robust per-sample loss computation with extensive NaN protection"""
        try:
            # Validate inputs first
            if torch.isnan(outputs.logits).any() or torch.isinf(outputs.logits).any():
                logger.warning("Invalid logits detected, using fallback losses")
                return [1.0] * labels.size(0)
            
            # Language modeling loss computation
            shift_logits = outputs.logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            
            batch_size, seq_len, vocab_size = shift_logits.shape
            
            # Enhanced dimension validation
            if batch_size == 0 or seq_len == 0 or vocab_size == 0:
                logger.error(f"Dimension issue: logits {shift_logits.shape}")
                return [1.0] * batch_size
            
            # Flatten for cross_entropy
            flat_logits = shift_logits.view(-1, vocab_size)
            flat_labels = shift_labels.view(-1)
            
            # Additional logits validation
            if torch.isnan(flat_logits).any() or torch.isinf(flat_logits).any():
                logger.warning("NaN/Inf in flattened logits")
                return [1.0] * batch_size
            
            # Compute token losses with enhanced error handling
            try:
                token_losses = F.cross_entropy(
                    flat_logits,
                    flat_labels,
                    ignore_index=-100,
                    reduction='none'
                )
                
                # Validate token losses
                if torch.isnan(token_losses).any() or torch.isinf(token_losses).any():
                    logger.warning("NaN/Inf in token losses")
                    return [1.0] * batch_size
                    
            except Exception as e:
                logger.error(f"Cross entropy computation failed: {e}")
                return [1.0] * batch_size
            
            # Reshape back
            token_losses = token_losses.view(batch_size, seq_len)
            
            sample_losses = []
            
            for i in range(batch_size):
                try:
                    # Find valid tokens
                    valid_mask = shift_labels[i] != -100
                    valid_count = valid_mask.sum().item()
                    
                    if valid_count > 0:
                        sample_token_losses = token_losses[i][valid_mask]
                        
                        # Additional validation
                        if torch.isnan(sample_token_losses).any() or torch.isinf(sample_token_losses).any():
                            logger.warning(f"Invalid token losses for sample {i}")
                            loss_value = 1.0
                        else:
                            sample_loss = sample_token_losses.mean()
                            
                            if torch.isnan(sample_loss) or torch.isinf(sample_loss):
                                logger.warning(f"Invalid mean loss for sample {i}: {sample_loss}")
                                loss_value = 1.0
                            else:
                                loss_value = sample_loss.item()
                                # Enhanced clamping
                                if loss_value <= 0 or loss_value > 20.0:
                                    logger.warning(f"Extreme loss value {loss_value} for sample {i}, clamping")
                                    loss_value = max(0.01, min(loss_value, 15.0))
                        
                        sample_losses.append(loss_value)
                    else:
                        # No valid tokens - use fallback
                        logger.warning(f"No valid tokens for sample {i}")
                        sample_losses.append(1.0)
                        
                except Exception as e:
                    logger.error(f"Error processing sample {i}: {e}")
                    sample_losses.append(1.0)
            
            return sample_losses
            
        except Exception as e:
            logger.error(f"Critical error in enhanced loss computation: {e}")
            # Return fallback losses for all samples
            return [1.0] * labels.size(0)
    
    def save_checkpoint(self, checkpoint_name, epoch, global_step, val_metrics=None):
        """Save a full checkpoint for resume capability"""
        checkpoint_path = os.path.join(self.checkpoint_dir, "checkpoints", f"{checkpoint_name}.pt")
        
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'epoch': epoch,
            'global_step': global_step,
            'config': TRAINING_CONFIG,
            'tokenizer_name': self.model_name,
            'timestamp': datetime.now().isoformat()
        }
        
        if val_metrics:
            checkpoint.update(val_metrics)
        
        # Save checkpoint
        temp_path = checkpoint_path + '.tmp'
        torch.save(checkpoint, temp_path)
        os.rename(temp_path, checkpoint_path)  # Atomic operation
        
        logger.info(f"üíæ Checkpoint saved: {checkpoint_path}")
        
        # Manage checkpoint count
        self._cleanup_old_checkpoints()
    
    def _cleanup_old_checkpoints(self):
        """Keep only the last N checkpoints"""
        checkpoint_dir = os.path.join(self.checkpoint_dir, "checkpoints")
        checkpoints = sorted([
            f for f in os.listdir(checkpoint_dir) 
            if f.endswith('.pt') and not f.startswith(('checkpoint-best', 'checkpoint-final'))
        ])
        
        if len(checkpoints) > TRAINING_CONFIG["keep_last_n_checkpoints"]:
            for old_checkpoint in checkpoints[:-TRAINING_CONFIG["keep_last_n_checkpoints"]]:
                os.remove(os.path.join(checkpoint_dir, old_checkpoint))
                logger.info(f"üóëÔ∏è Removed old checkpoint: {old_checkpoint}")
    
    def save_model(self, checkpoint_name):
        """‚úÖ FIXED SAVE METHOD - WITHOUT DTensor dependencies"""
        output_dir = os.path.join(self.checkpoint_dir, checkpoint_name, "model")
        
        logger.info(f"üíæ Attempting to save model to: {output_dir}")
        
        try:
            # Create directory
            os.makedirs(output_dir, exist_ok=True)
            
            # Method 1: Try standard safe save
            try:
                logger.info("   Trying standard safe save...")
                self.model.save_pretrained(
                    output_dir, 
                    safe_serialization=True,
                    max_shard_size="2GB"
                )
                self.tokenizer.save_pretrained(output_dir)
                logger.info("‚úÖ Standard safe save successful!")
                
                # Save training config
                config_path = os.path.join(output_dir, "training_config.json")
                with open(config_path, 'w') as f:
                    json.dump(TRAINING_CONFIG, f, indent=2)
                
                return True
                
            except Exception as e1:
                logger.warning(f"   Standard safe save failed: {e1}")
                
                # Method 2: Try without safe serialization
                try:
                    logger.info("   Trying without safe serialization...")
                    self.model.save_pretrained(
                        output_dir,
                        safe_serialization=False,
                        max_shard_size="2GB"
                    )
                    self.tokenizer.save_pretrained(output_dir)
                    logger.info("‚úÖ Non-safe save successful!")
                    
                    # Save config
                    config_path = os.path.join(output_dir, "training_config.json")
                    with open(config_path, 'w') as f:
                        json.dump(TRAINING_CONFIG, f, indent=2)
                    
                    return True
                    
                except Exception as e2:
                    logger.warning(f"   Non-safe save failed: {e2}")
                    
                    # Method 3: Manual state dict save
                    try:
                        logger.info("   Trying manual state dict save...")
                        
                        # Save model state dict
                        model_path = os.path.join(output_dir, "pytorch_model.bin")
                        torch.save(self.model.state_dict(), model_path)
                        
                        # Save config manually
                        config_dict = self.model.config.to_dict()
                        config_path = os.path.join(output_dir, "config.json")
                        with open(config_path, 'w') as f:
                            json.dump(config_dict, f, indent=2)
                        
                        # Save tokenizer
                        self.tokenizer.save_pretrained(output_dir)
                        
                        # Save training config
                        train_config_path = os.path.join(output_dir, "training_config.json")
                        with open(train_config_path, 'w') as f:
                            json.dump(TRAINING_CONFIG, f, indent=2)
                        
                        logger.info("‚úÖ Manual save successful!")
                        return True
                        
                    except Exception as e3:
                        logger.error(f"   Manual save failed: {e3}")
                        return False
                        
        except Exception as e:
            logger.error(f"‚ùå All save methods failed: {e}")
            return False
    
    def evaluate_model(self, val_loader, epoch, step, eval_num):
        """Model evaluation for SFT-only"""
        self.model.eval()
        
        total_loss = 0
        all_losses = []
        num_batches = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                if torch.cuda.is_available():
                    input_ids = batch['input_ids'].cuda(non_blocking=True)
                    attention_mask = batch['attention_mask'].cuda(non_blocking=True)
                    labels = batch['labels'].cuda(non_blocking=True)
                else:
                    input_ids = batch['input_ids']
                    attention_mask = batch['attention_mask']
                    labels = batch['labels']
                
                # ‚úÖ NO MIXED PRECISION IN EVALUATION
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                sample_losses = self.compute_per_sample_losses_enhanced(outputs, labels)
                all_losses.extend(sample_losses)
                
                total_loss += outputs.loss.item()
                num_batches += 1
        
        self.model.train()
        
        if num_batches > 0:
            avg_loss = total_loss / num_batches
            sample_avg = sum(all_losses) / len(all_losses) if all_losses else 0.0
            
            logger.info(f"\n{'='*60}")
            logger.info(f"üìä VALIDATION {eval_num}/{TRAINING_CONFIG['eval_frequency']} - Epoch {epoch+1}, Step {step}")
            logger.info(f"{'='*60}")
            logger.info(f"  ‚Ä¢ Average Loss: {avg_loss:.4f}")
            logger.info(f"  ‚Ä¢ Sample Average: {sample_avg:.4f} ({len(all_losses)} samples)")
            
            # Update early stopping
            is_new_best = self.early_stopping.update(
                avg_loss, None, epoch, step  # model_state will be set inside if new best
            )
            
            # Only copy model state if it's a new best
            if is_new_best:
                model_state = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}
                self.early_stopping.best_model_state = model_state
                logger.info(f"‚úÖ New best model saved in memory")
            
            # Show early stopping status
            status = self.early_stopping.get_status()
            logger.info(f"  ‚Ä¢ Early Stop: {status['patience']}")
            logger.info(f"  ‚Ä¢ Best Loss: {status['best_loss']:.4f} (epoch {status['best_epoch']}, step {status['best_step']})")
            logger.info(f"{'='*60}\n")
            
            return {
                'avg_loss': avg_loss,
                'is_new_best': is_new_best
            }
        
        return None
    
    def train_sft(self, train_dataset, val_dataset):
        """SFT-only training"""
        config = TRAINING_CONFIG
        
        self.setup_optimizer(train_dataset)
        val_loader = self.create_dataloader(val_dataset, is_training=False)
        
        # Check for checkpoint resume
        start_epoch = 0
        global_step = 0
        
        if config["resume_from_checkpoint"]:
            checkpoint_path = config["resume_from_checkpoint"]
            if os.path.exists(checkpoint_path):
                start_epoch, global_step = self.load_checkpoint(checkpoint_path)
            else:
                logger.warning(f"‚ö†Ô∏è Checkpoint not found: {checkpoint_path}")
                logger.info("Starting from scratch...")
        
        best_loss = float('inf')
        
        logger.info("üöÄ Starting SFT-only training...")
        logger.info(f"   üìä Dataset: codexglue_train.json")
        logger.info(f"   üéØ Each sample appears exactly once per epoch")
        logger.info(f"   üîß Early stopping (patience={config['early_stopping_patience']})")
        logger.info(f"   üìà {config['eval_frequency']} evaluations per epoch")
        logger.info(f"   üíæ Best model saved based on validation loss")
        if start_epoch > 0:
            logger.info(f"   üîÑ Resuming from epoch {start_epoch}, step {global_step}")
        
        training_start_time = time.time()
        
        for epoch in range(start_epoch, config["num_epochs"]):
            logger.info(f"\nüéØ EPOCH {epoch + 1}/{config['num_epochs']}")
            
            # Create new dataloader for shuffling
            train_loader = self.create_dataloader(train_dataset, is_training=True)
            
            self.model.train()
            accumulated_loss = 0
            processed_batches = 0
            eval_counter = 1
            
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
            
            epoch_start_time = time.time()
            
            for batch_idx, batch in enumerate(progress_bar):
                batch_start_time = time.time()
                
                # ‚úÖ FIXED: Compute losses with enhanced NaN protection, NO SKIPPING
                try:
                    sample_losses, combined_loss = self.compute_loss(batch)
                    
                    # Apply gradient accumulation scaling
                    loss = combined_loss / config["gradient_accumulation_steps"]
                    
                    # Final NaN check before backward
                    if torch.isnan(loss) or torch.isinf(loss):
                        logger.warning(f"NaN loss after scaling in batch {batch_idx}, using emergency fallback")
                        loss = torch.tensor(0.1, device=loss.device, dtype=torch.float32, requires_grad=True)
                    
                    loss.backward()
                    accumulated_loss += loss.item()
                    processed_batches += 1
                    
                except Exception as e:
                    logger.error(f"‚ùå Critical error in batch {batch_idx}: {e}")
                    # ‚úÖ FIXED: Don't skip, create emergency fallback loss
                    emergency_loss = torch.tensor(0.2, device=next(self.model.parameters()).device, dtype=torch.float32, requires_grad=True)
                    loss = emergency_loss / config["gradient_accumulation_steps"]
                    loss.backward()
                    accumulated_loss += loss.item()
                    processed_batches += 1
                    logger.warning(f"   Using emergency fallback loss, continuing training...")
                
                # Gradient step
                if (batch_idx + 1) % config["gradient_accumulation_steps"] == 0:
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), config["max_grad_norm"])
                    
                    self.optimizer.step()
                    self.scheduler.step()
                    self.optimizer.zero_grad(set_to_none=True)
                    
                    global_step += 1
                    
                    # Memory cleanup
                    if global_step % self.cleanup_frequency == 0:
                        torch.cuda.empty_cache()
                        gc.collect()
                    
                    # Periodic checkpoint
                    if global_step % config["save_every_n_steps"] == 0:
                        self.save_checkpoint(f"checkpoint-step-{global_step}", epoch, global_step)
                    
                    # Evaluation
                    if global_step % self.eval_steps == 0:
                        val_metrics = self.evaluate_model(val_loader, epoch, global_step, eval_counter)
                        eval_counter += 1
                        
                        if val_metrics and val_metrics['is_new_best']:
                            logger.info(f"üèÜ New best model! Saving...")
                            self.save_model("best")
                            self.save_checkpoint("checkpoint-best", epoch, global_step, val_metrics)
                        
                        # Check early stopping
                        if self.early_stopping.should_stop():
                            logger.info(f"üõë Early stopping triggered!")
                            logger.info(f"   Loss stopped improving for {config['early_stopping_patience']} evaluations")
                            
                            # Save final model
                            self.save_model("final")
                            self.save_checkpoint("checkpoint-final", epoch, global_step)
                            
                            # Load and save best model
                            if self.early_stopping.best_model_state:
                                logger.info(f"üìÇ Loading best model from epoch {self.early_stopping.best_epoch}, step {self.early_stopping.best_step}")
                                self.model.load_state_dict(self.early_stopping.best_model_state)
                                self.save_model("best")
                            
                            return  # Stop training
                    
                    # Logging
                    if global_step % config["logging_steps"] == 0:
                        avg_loss = self.loss_tracker.get_recent_average()
                        current_lr = self.scheduler.get_last_lr()[0]
                        
                        if torch.cuda.is_available():
                            vram_used = torch.cuda.memory_allocated() / 1024**3
                            vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                            vram_util = vram_used / vram_total * 100
                        else:
                            vram_used = vram_total = vram_util = 0
                        
                        logger.info(f"üöÄ Step {global_step:4d} | Loss: {avg_loss:.4f} | LR: {current_lr:.2e}")
                        if torch.cuda.is_available():
                            logger.info(f"   GPU: {vram_used:.1f}GB/{vram_total:.1f}GB ({vram_util:.1f}%)")
                
                batch_time = time.time() - batch_start_time
                
                # Update progress bar
                if global_step % 5 == 0:
                    avg_loss = self.loss_tracker.get_recent_average()
                    samples_per_sec = config["batch_size"] / batch_time if batch_time > 0 else 0
                    
                    progress_bar.set_postfix({
                        'Loss': f'{avg_loss:.3f}',
                        'S/s': f'{samples_per_sec:.1f}'
                    })
            
            # End-of-epoch checkpoint
            epoch_time = time.time() - epoch_start_time
            logger.info(f"üíæ Saving epoch {epoch+1} checkpoint...")
            self.save_checkpoint(f"checkpoint-epoch-{epoch+1}", epoch, global_step)
            
            logger.info(f"üéØ EPOCH {epoch + 1} COMPLETED")
            logger.info(f"   Time: {epoch_time/60:.1f} min")
            logger.info(f"   Processed batches: {processed_batches}")
            logger.info(f"   Global steps: {global_step}")
        
        total_training_time = time.time() - training_start_time
        
        # Save final model
        logger.info("üíæ Saving final model...")
        self.save_model("final")
        self.save_checkpoint("checkpoint-final", config["num_epochs"]-1, global_step)
        
        # Save best model if exists
        if self.early_stopping.best_model_state:
            logger.info("üíæ Saving best model...")
            self.model.load_state_dict(self.early_stopping.best_model_state)
            self.save_model("best")
        
        # Final summary
        logger.info("\nüöÄ TRAINING SUMMARY:")
        logger.info(f"   ‚Ä¢ Total time: {total_training_time/3600:.1f} hours")
        logger.info(f"   ‚Ä¢ Total epochs: {config['num_epochs']}")
        logger.info(f"   ‚Ä¢ Total steps: {global_step}")
        logger.info(f"   ‚Ä¢ Best model: epoch {self.early_stopping.best_epoch}, step {self.early_stopping.best_step}")
        logger.info(f"   ‚Ä¢ Best loss: {self.early_stopping.best_loss:.4f}")
        logger.info(f"   ‚Ä¢ Final structure:")
        logger.info(f"     - {self.checkpoint_dir}/best/model/")
        logger.info(f"     - {self.checkpoint_dir}/final/model/")
        logger.info(f"     - {self.checkpoint_dir}/checkpoints/")

def main():
    """Main training function"""
    print("üöÄ SFT-Only Training")
    print("üîß Features:")
    print("   ‚úÖ Simple SFT dataset: each sample once per epoch")
    print("   ‚úÖ 10% validation split")
    print("   ‚úÖ Early stopping (patience=3)")
    print("   ‚úÖ 5 evaluations per epoch")
    print("   ‚úÖ Best model tracking and checkpointing")
    print("   ‚úÖ Float32 precision for stability")
    print("   ‚úÖ NaN protection throughout")
    print("   ‚úÖ No batch skipping")
    print("   ‚úÖ Resume from checkpoint support")
    
    # Set seeds
    torch.manual_seed(TRAINING_CONFIG["seed"])
    np.random.seed(TRAINING_CONFIG["seed"])
    random.seed(TRAINING_CONFIG["seed"])
    
    # Enable optimizations
    if torch.cuda.is_available():
        try:
            torch.backends.cuda.enable_flash_sdp(True)
            print("‚úÖ Flash Attention enabled")
        except:
            print("‚ö†Ô∏è  Flash Attention not available")
    
    # Check GPU
    if not torch.cuda.is_available():
        logger.warning("‚ö†Ô∏è  CUDA not available! Running on CPU.")
        print("‚ö†Ô∏è  CUDA not available! Running on CPU.")
    else:
        gpu_name = torch.cuda.get_device_name(0)
        total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"\nüí™ GPU DETECTED:")
        print(f"   üéÆ GPU: {gpu_name}")
        print(f"   üíæ VRAM: {total_vram:.1f} GB")
        print(f"   üßÆ CUDA: {torch.version.cuda}")
    
    print(f"\nüéØ TRAINING CONFIGURATION:")
    print(f"   üìä Dataset: codexglue_train.json")
    print(f"   üéØ Epochs: {TRAINING_CONFIG['num_epochs']}")
    print(f"   üîß Batch size: {TRAINING_CONFIG['batch_size']} x {TRAINING_CONFIG['gradient_accumulation_steps']} = {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']} effective")
    print(f"   üìà Evaluations per epoch: {TRAINING_CONFIG['eval_frequency']}")
    print(f"   üõë Early stopping patience: {TRAINING_CONFIG['early_stopping_patience']}")
    print(f"   üíæ Max length: {TRAINING_CONFIG['max_length']} tokens")
    print(f"   üõ°Ô∏è  Precision: float32 (maximum stability)")
    
    # Check for checkpoint resume
    if TRAINING_CONFIG["resume_from_checkpoint"]:
        print(f"\nüîÑ RESUME MODE:")
        print(f"   üìÇ Checkpoint: {TRAINING_CONFIG['resume_from_checkpoint']}")
    
    try:
        trainer = A100SFTTrainer()
        
        print("\nü§ñ Loading model with float32 precision...")
        trainer.setup_model_and_tokenizer()
        
        print("\nüìä Preparing SFT dataset...")
        train_dataset, val_dataset = trainer.prepare_dataset()
        
        print(f"\nüéØ Training Configuration Summary:")
        print(f"   üìö Train samples: {len(train_dataset)}")
        print(f"   üß™ Val samples: {len(val_dataset)}")
        print(f"   üõ°Ô∏è  Float32 precision training")
        print(f"   ‚ö° Pre-tokenized data")
        print(f"   üîß Enhanced NaN protection")
        print(f"   üõë Early stopping")
        print(f"   üíæ Checkpoint directory: {TRAINING_CONFIG['checkpoint_dir']}")
        
        print("\nüèãÔ∏è Starting SFT training...")
        trainer.train_sft(train_dataset, val_dataset)
        
        print("\n‚úÖ Training completed successfully!")
        
        # Check final results
        best_dir = os.path.join(TRAINING_CONFIG['checkpoint_dir'], "best/model")
        final_dir = os.path.join(TRAINING_CONFIG['checkpoint_dir'], "final/model")
        
        print("\nüéØ FINAL RESULTS:")
        if os.path.exists(best_dir):
            print(f"üèÜ Best model: ‚úÖ Saved at {best_dir}")
        else:
            print("üèÜ Best model: ‚ùå Not saved")
            
        if os.path.exists(final_dir):
            print(f"üöÄ Final model: ‚úÖ Saved at {final_dir}")
        else:
            print("üöÄ Final model: ‚ùå Not saved")
        
    except Exception as e:
        logger.error(f"Training failed: {e}")
        import traceback
        traceback.print_exc()
        
        logger.error("üîç Debug Information:")
        logger.error(f"   ‚Ä¢ Current directory: {os.getcwd()}")
        logger.error(f"   ‚Ä¢ Dataset exists: {os.path.exists('/your/base/path/codexglue_train.json')}")  # Replace path
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.error(f"   ‚Ä¢ GPU: {gpu_name}")
            logger.error(f"   ‚Ä¢ VRAM: {total_vram:.1f} GB")
        else:
            logger.error("   ‚Ä¢ CUDA: Not available")
    
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        print("\nüßπ Cleanup completed")

if __name__ == "__main__":
    main()